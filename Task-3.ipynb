{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016b63e-7f62-452c-b895-d65cb54ce8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow numpy opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba220e07-c96b-4969-92f9-80f656f54a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load ResNet50 model pre-trained on ImageNet\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_img(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Function to extract features from the image\n",
    "def extract_features(img_path):\n",
    "    img = preprocess_img(img_path)\n",
    "    features = model.predict(img)\n",
    "    return features\n",
    "\n",
    "# Example: Extract features from a sample image\n",
    "features = extract_features('sample.jpg')\n",
    "print(features.shape)  # Should output (1, 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e17ab2-1b9c-46c0-9119-9cc099f71e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare the Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66e62d-3f95-433c-98d6-5fdc8cedc14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample captions\n",
    "captions = [\"A cat is sitting on a mat\", \"A dog is playing with a ball\"]\n",
    "\n",
    "# Preprocess captions\n",
    "def preprocess_captions(captions):\n",
    "    captions = [caption.lower().translate(str.maketrans('', '', string.punctuation)) for caption in captions]\n",
    "    return captions\n",
    "\n",
    "captions = preprocess_captions(captions)\n",
    "\n",
    "# Tokenize captions\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(captions)\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e55556-8502-4b03-ad66-1242108a6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2bc8b-d0ef-4163-aa1a-2a91afb818c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Image feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# Sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(input_dim=5000, output_dim=256, mask_zero=True)(inputs2)\n",
    "se2 = LSTM(256)(se1)\n",
    "\n",
    "# Decoder model\n",
    "decoder1 = add([fe2, se2])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(5000, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771102f-bb4d-421c-bd63-317413253c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba681893-992e-4919-860f-abea4070dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Image feature extractor model\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# Sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(input_dim=5000, output_dim=256, mask_zero=True)(inputs2)\n",
    "se2 = LSTM(256)(se1)\n",
    "\n",
    "# Decoder model\n",
    "decoder1 = add([fe2, se2])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(5000, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fac789-01b1-4762-a85e-52e9f6112955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERATE CAPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87564079-b01c-4f3e-8b39-947b93880324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_features, tokenizer, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([img_features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word[yhat]\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# Generate caption for a new image\n",
    "new_features = extract_features('new_image.jpg')\n",
    "caption = generate_caption(model, new_features, tokenizer, max_length)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
